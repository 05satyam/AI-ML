{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPQ0P/YKZpCsTRtfAbebckn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "40181fd77ec64d20ba621c070a121179": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7e67487830444ca986e8eb0c2b7d5744",
              "IPY_MODEL_533c169914554155bc2eb75d53959686",
              "IPY_MODEL_d751cefa5bb748e3944fd8ebefe78011"
            ],
            "layout": "IPY_MODEL_7dac85884282414b8d1979666d05dade"
          }
        },
        "7e67487830444ca986e8eb0c2b7d5744": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad10e112053647dda0ccfce06dccc558",
            "placeholder": "​",
            "style": "IPY_MODEL_eb33ac52a57b43b4985aca9184221dc1",
            "value": "Computing transition probabilities: 100%"
          }
        },
        "533c169914554155bc2eb75d53959686": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb392ca68912455ca92edcbe73eb15ae",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_031df3e39d1c463785bb577a047dbc48",
            "value": 4
          }
        },
        "d751cefa5bb748e3944fd8ebefe78011": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a39b0a3234904615b1bd625d91f27b59",
            "placeholder": "​",
            "style": "IPY_MODEL_9ecc625fd0dc4ee1ac0c9d302c0d18f3",
            "value": " 4/4 [00:00&lt;00:00, 200.44it/s]"
          }
        },
        "7dac85884282414b8d1979666d05dade": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad10e112053647dda0ccfce06dccc558": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eb33ac52a57b43b4985aca9184221dc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb392ca68912455ca92edcbe73eb15ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "031df3e39d1c463785bb577a047dbc48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a39b0a3234904615b1bd625d91f27b59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ecc625fd0dc4ee1ac0c9d302c0d18f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/05satyam/AI-ML/blob/main/Graph_Based_Retrieval_Augmented_Generation_(RAG)_System_Using_Networkx%2CLangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, I've implemented a graph-based Retrieval-Augmented Generation (RAG) system using LangChain. By building a knowledge graph from our documents and creating a custom retriever that leverages graph embeddings, we've enhanced the retrieval process. The retrieved context is then used by the language model to generate accurate and contextually relevant answers.\n",
        "\n"
      ],
      "metadata": {
        "id": "4jOTxXG3IRf1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INTRODUCTION"
      ],
      "metadata": {
        "id": "QdxRdR5FIgXi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Retrieval-Augmented Generation (RAG)**\n",
        " - RAG is a powerful framework that enhances the capabilities of generative models by incorporating external knowledge retrieved from a dataset. By integrating graph-based methods, you can capture complex relationships within your data, leading to more accurate and contextually relevant outputs.\n",
        "\n",
        "- This guide will help you build a graph-based RAG system from scratch, combining retrieval techniques with advanced graph representations.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**RAG Framework Overview**\n",
        "\n",
        "Retrieval Component: Searches a knowledge base to find relevant documents or data pieces related to an input query.\n",
        "Generation Component: Uses a generative model (like GPT or T5) to produce responses conditioned on the retrieved information.\n",
        "Benefits of RAG\n",
        "\n",
        "Improved Accuracy: Incorporates up-to-date and specific information.\n",
        "Contextual Responses: Generates answers that are more relevant to the query context.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**Advantages of Graph-Based Retrieval**\n",
        "\n",
        "Capturing Relationships: Graphs represent entities and their relationships, enabling the model to understand context better.\n",
        "Semantic Understanding: Graph structures can encode semantic similarities and hierarchies.\n",
        "Efficient Traversal: Graph algorithms can efficiently find relevant subgraphs or paths related to a query.\n",
        "\n",
        "---\n",
        "\n",
        "**Building the Graph**\n",
        " - a. Define Graph Components\n",
        "\n",
        "    - Nodes: Represent entities, concepts, or documents.\n",
        "    - Edges: Represent relationships between nodes (e.g., \"author of,\" \"related to\").\n",
        "\n",
        " - b. Construct the Graph from scratch\n",
        "Use NetworkX or DGL to build the graph structure.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "### Technical Requirements\n",
        " - Programming Language: Python 3.7 or higher.\n",
        " - Libraries and Tools:\n",
        "    - langchain\n",
        "    - networkx for graph operations.\n",
        "    - Build graph using networkx or use neo4j or graphdb for graph databases (optional).\n",
        "    - transformers for language models.\n",
        "    - faiss or annoy for similarity search (optional).\n",
        " - Accounts:\n",
        "   - OpenAI API key or access to another LLM provider supported by LangChain.\n"
      ],
      "metadata": {
        "id": "GJnMKzurIjJS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pb7h0pvLycIX"
      },
      "outputs": [],
      "source": [
        "%pip install langchain==0.0.200\n",
        "\n",
        "%pip install networkx #for graph operations\n",
        "%pip install faiss-cpu\n",
        "%pip install node2vec\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "HtYy63ipyfXq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import spacy\n",
        "import faiss\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "from typing import List\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "from langchain.schema import BaseRetriever\n"
      ],
      "metadata": {
        "id": "g_S8JwzczALl"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "documents = [\n",
        "    \"Transformers are a type of neural network architecture that have been used for NLP tasks.\",\n",
        "    \"Graph Neural Networks can capture dependencies in graph-structured data.\",\n",
        "    \"The GPT models are developed by OpenAI and are used for text generation.\",\n",
        "    \"Machine learning involves training models on data.\",\n",
        "    \"Deep learning is a subset of machine learning that uses neural networks.\"\n",
        "]\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Preprocess documents and extract entities\n",
        "processed_docs = []\n"
      ],
      "metadata": {
        "id": "7zjXOYxKzGFz"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in documents:\n",
        "  spacy_doc=nlp(doc)\n",
        "  entities = [(ent.text, ent.label_) for ent in spacy_doc.ents]\n",
        "  print(entities)\n",
        "  processed_docs.append({'text': doc, 'entities': entities})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_7YsL7kzLqv",
        "outputId": "4b1b81cb-4b04-4b69-b429-338c2bf0c2cb"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLP', 'ORG')]\n",
            "[('Graph Neural Networks', 'ORG')]\n",
            "[('GPT', 'ORG'), ('OpenAI', 'GPE')]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " # Building the Knowledge Graph"
      ],
      "metadata": {
        "id": "kIo61IGlzgat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "G=nx.Graph()"
      ],
      "metadata": {
        "id": "R8I9Mf6yzjSj"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Add Nodes and Edges"
      ],
      "metadata": {
        "id": "OIS-5YUmzoTK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for doc in processed_docs:\n",
        "  entities=doc['entities']\n",
        "  print(entities)\n",
        "  # Add nodes\n",
        "  for entity in entities:\n",
        "    G.add_node(entity[0], label=entity[1])\n",
        "\n",
        "  # Add edges\n",
        "  for i in range(len(entities)):\n",
        "    for j in range(i+1, len(entities)):\n",
        "      G.add_edge(entities[i][0], entities[j][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xeWFd0NzpIY",
        "outputId": "8a1d6bbc-7712-4f27-f490-7a5a91cd66c9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('NLP', 'ORG')]\n",
            "[('Graph Neural Networks', 'ORG')]\n",
            "[('GPT', 'ORG'), ('OpenAI', 'GPE')]\n",
            "[]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualize the Graph (Optional)"
      ],
      "metadata": {
        "id": "Aa2i1Wtf0P-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(3,3))\n",
        "pos=nx.spring_layout(G)\n",
        "nx.draw(G, pos, with_labels=True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337
        },
        "id": "lJ6e-eWZ0Q6U",
        "outputId": "13d47d01-982e-4c9d-bf9b-35b95c5ff28e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 300x300 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the Graph-Based Retriever"
      ],
      "metadata": {
        "id": "QHaznAK90lbU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " - Use Node2Vec to generate embeddings for each node in the graph.\n",
        " - Create an embedding matrix and a mapping from node names to indices.\n",
        " - Build the Faiss Index\n",
        " - Implement the Custom Retriever"
      ],
      "metadata": {
        "id": "8FySBxBK0ptd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Details about the parameters selected for node2vec\n",
        "\n",
        "### Understanding Node2Vec Parameters\n",
        "\n",
        "**Node2Vec** is an algorithm designed to learn continuous feature representations for nodes in a graph by simulating biased random walks and then applying the Skip-Gram model from **word2vec**. The main idea is to capture the network's topology and structure in the embeddings.\n",
        "\n",
        "#### Parameters in `Node2Vec` Constructor\n",
        "\n",
        "##### `dimensions=64`\n",
        "\n",
        "- **What it is**: Specifies the size of the embedding vectors (i.e., the number of features in the embedding space).\n",
        "- **Why 64**: A dimension of 64 is a common default that balances embedding quality and computational efficiency.\n",
        "- **Other possible values**: Typical values are 32, 64, 128, or 256.\n",
        "- **Criteria for choosing**:\n",
        "  - **Graph Complexity**: More complex graphs may benefit from higher-dimensional embeddings to capture intricate relationships.\n",
        "  - **Computational Resources**: Higher dimensions increase memory usage and computation time.\n",
        "  - **Downstream Tasks**: If your application (e.g., node classification, link prediction) requires capturing subtle differences between nodes, higher dimensions may be beneficial.\n",
        "\n",
        "##### `walk_length=20`\n",
        "\n",
        "- **What it is**: The number of nodes visited during each random walk.\n",
        "- **Why 20**: A walk length of 20 allows the random walk to capture both local and some global structures in the graph.\n",
        "- **Other possible values**: Values can range from 10 to 80 or more.\n",
        "- **Criteria for choosing**:\n",
        "  - **Graph Diameter**: If the graph has a large diameter (nodes are far apart), longer walks can capture more global information.\n",
        "  - **Local vs. Global Structure**: Shorter walks focus on local neighborhoods; longer walks include more distant nodes.\n",
        "  - **Performance**: Longer walks increase computation time and memory usage.\n",
        "\n",
        "##### `num_walks=100`\n",
        "\n",
        "- **What it is**: The number of walks to start from each node.\n",
        "- **Why 100**: Starting 100 walks from each node ensures a good coverage of the graph's structure.\n",
        "- **Other possible values**: Commonly range from 10 to several hundred.\n",
        "- **Criteria for choosing**:\n",
        "  - **Graph Size**: Larger graphs may require more walks to adequately sample the space.\n",
        "  - **Variance Reduction**: More walks can reduce variance in the embeddings.\n",
        "  - **Computation Time**: Increasing the number of walks increases computation time proportionally.\n",
        "\n",
        "##### `workers=4`\n",
        "\n",
        "- **What it is**: The number of parallel threads used for computation.\n",
        "- **Why 4**: Likely corresponds to the number of CPU cores available, allowing efficient parallel processing.\n",
        "- **Other possible values**: Any positive integer up to the number of CPU cores.\n",
        "- **Criteria for choosing**:\n",
        "  - **Hardware Availability**: Use the number of cores available to maximize performance.\n",
        "  - **System Load**: Be mindful of other processes; over-allocating can lead to system slowdowns.\n",
        "\n",
        "---\n",
        "\n",
        "#### Parameters in `model.fit` Method\n",
        "\n",
        "The `fit` method applies the Skip-Gram model to the generated random walks to learn embeddings.\n",
        "\n",
        "##### `window=10`\n",
        "\n",
        "- **What it is**: The maximum distance between the current node and predicted nodes within a random walk.\n",
        "- **Why 10**: A larger window size captures broader context from each random walk.\n",
        "- **Other possible values**: Typically between 2 and 10, but can be larger.\n",
        "- **Criteria for choosing**:\n",
        "  - **Contextual Information**: Larger windows consider more nodes as context, potentially capturing more structural information.\n",
        "  - **Computational Cost**: Larger windows increase the number of training examples, impacting training time.\n",
        "\n",
        "##### `min_count=1`\n",
        "\n",
        "- **What it is**: Ignores all nodes with total frequency lower than this threshold during training.\n",
        "- **Why 1**: Setting it to 1 ensures that all nodes are included in the training, even those that appear infrequently.\n",
        "- **Other possible values**: Values greater than 1 can exclude rare nodes.\n",
        "- **Criteria for choosing**:\n",
        "  - **Data Sparsity**: If some nodes are extremely rare and potentially noisy, increasing `min_count` can improve embedding quality.\n",
        "  - **Graph Coverage**: Setting `min_count` too high may exclude important nodes.\n",
        "\n",
        "##### `batch_words=4`\n",
        "\n",
        "- **What it is**: The target size (in words) for batches of examples passed to worker threads.\n",
        "- **Why 4**: A small batch size can be suitable for systems with limited memory or for smaller datasets.\n",
        "- **Other possible values**: Commonly set to 10,000 or larger.\n",
        "- **Criteria for choosing**:\n",
        "  - **Memory Constraints**: Larger batch sizes require more memory.\n",
        "  - **Training Efficiency**: Larger batches can improve computational efficiency due to vectorization.\n",
        "  - **Convergence Stability**: Smaller batches can provide more stable updates but may take longer to converge.\n",
        "\n",
        "---\n",
        "\n",
        "#### Practical Tips\n",
        "\n",
        "- **Start with Default Values**: If you're unsure, begin with commonly used defaults and adjust from there.\n",
        "- **Monitor Training**: Keep an eye on training time and resource usage; adjust parameters if training takes too long or uses too much memory.\n",
        "- **Visualize Embeddings**: Use dimensionality reduction techniques (e.g., t-SNE, UMAP) to visualize embeddings and assess their quality.\n",
        "- **Validate on Downstream Tasks**: Evaluate how well the embeddings perform on tasks like node classification to guide parameter selection.\n"
      ],
      "metadata": {
        "id": "a5ZBJ_kD3vYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from node2vec import Node2Vec\n",
        "node2vec=Node2Vec(G, dimensions=64, walk_length=20, num_walks=100, workers=4)\n",
        "n2v_model=node2vec.fit(window=10, min_count=1, batch_words=4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "40181fd77ec64d20ba621c070a121179",
            "7e67487830444ca986e8eb0c2b7d5744",
            "533c169914554155bc2eb75d53959686",
            "d751cefa5bb748e3944fd8ebefe78011",
            "7dac85884282414b8d1979666d05dade",
            "ad10e112053647dda0ccfce06dccc558",
            "eb33ac52a57b43b4985aca9184221dc1",
            "eb392ca68912455ca92edcbe73eb15ae",
            "031df3e39d1c463785bb577a047dbc48",
            "a39b0a3234904615b1bd625d91f27b59",
            "9ecc625fd0dc4ee1ac0c9d302c0d18f3"
          ]
        },
        "id": "fhaIfScg05sh",
        "outputId": "a636ff0e-c722-4d7a-beea-54627d88b3a5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Computing transition probabilities:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40181fd77ec64d20ba621c070a121179"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "node2idx={node:idx for idx, node in enumerate(G.nodes())}\n",
        "idx2node={idx:node for node, idx in node2idx.items()}\n",
        "\n",
        "#create embeddings\n",
        "embeddings_dim=n2v_model.vector_size\n",
        "embedding_matrix=np.zeros((len(G.nodes()), embeddings_dim))\n",
        "for idx, node in idx2node.items():\n",
        "  embedding_matrix[idx]=n2v_model.wv[node]"
      ],
      "metadata": {
        "id": "wTELyNao0zcs"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- **IndexFlatL2** is a type of index in Faiss that stores all vectors in their raw form and performs exact nearest neighbor searches. One can also use IndexHNSW\n",
        "\n",
        "---\n",
        "\n",
        "- **Flat** means the index does not use any compression or approximation; it computes exact distances, which can be computationally intensive for large datasets.\n"
      ],
      "metadata": {
        "id": "P83pNBLj5bf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "#build faiss index\n",
        "index=faiss.IndexFlatL2(embeddings_dim)\n",
        "index.add(embedding_matrix)\n"
      ],
      "metadata": {
        "id": "6mSx19mQ0mIH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Implement the Custom Retriever that uses the graph embeddings."
      ],
      "metadata": {
        "id": "sOM8b33451Iz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.schema import Document\n",
        "class GraphRetriever(BaseRetriever):\n",
        "  def __init__(self, nlp, index, n2v_model, node2index, processed_docs):\n",
        "        self.nlp = nlp\n",
        "        self.index = index\n",
        "        self.n2v_model = n2v_model\n",
        "        self.node2index = node2index\n",
        "        self.processed_docs = processed_docs\n",
        "  def aget_relevant_documents(self): pass\n",
        "  def get_relevant_documents(self, query)->List[str]:\n",
        "    query_doc=self.nlp(query)\n",
        "    query_entities = [ent.text for ent in query_doc.ents if ent.text in self.n2v_model.wv]\n",
        "    if not query_entities:\n",
        "            return []\n",
        "    query_embeddings=np.array([self.n2v_model.wv[ent] for ent in query_entities]).astype('float32')\n",
        "    avg_query_embedding = np.mean(query_embeddings, axis=0).reshape(1, -1)\n",
        "    # Search in the index\n",
        "    k = 5  # Number of nearest neighbors\n",
        "    distances, indices = self.index.search(avg_query_embedding, k)\n",
        "    retrieved_nodes = [idx2node[idx] for idx in indices[0] if idx != -1]\n",
        "\n",
        "    # Retrieve documents containing these nodes\n",
        "    relevent_docs=[]\n",
        "    # print(f\"in retriever class : {retrieved_nodes}\")\n",
        "    for node in retrieved_nodes:\n",
        "      for doc in self.processed_docs:\n",
        "        # print(f\"doc: {doc}. , node={node}\" )\n",
        "        if node in [ent[0] for ent in doc['entities']]:\n",
        "            document = Document(\n",
        "                        page_content=doc['text'],\n",
        "                        metadata={'source_node': node}\n",
        "                    )\n",
        "            relevent_docs.append(document)\n",
        "    # Remove duplicates\n",
        "    # print(f\"in retriver class relevent_docs:{relevent_docs}\")\n",
        "    unique_docs = {doc.page_content: doc for doc in relevent_docs}\n",
        "\n",
        "    return list(unique_docs.values())\n",
        "\n"
      ],
      "metadata": {
        "id": "a1YyF7OF54Yd"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Integrating with the Language Model"
      ],
      "metadata": {
        "id": "0_1V61MP7-4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Initialize the Language Model\n",
        "## Create the RetrievalQA Chain\n",
        "## Customize the Prompt Template (Optional)"
      ],
      "metadata": {
        "id": "rcPZmCGA7_pi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "from langchain.llms import OpenAI\n",
        "retriever = GraphRetriever(\n",
        "    nlp=nlp,\n",
        "    index=index,\n",
        "    n2v_model=n2v_model,\n",
        "    node2index=node2idx,\n",
        "    processed_docs=processed_docs\n",
        ")\n",
        "# qa_chain = RetrievalQA(llm=llm, retriever=retriever)"
      ],
      "metadata": {
        "id": "c-cH3z_W8Fug"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"YOUR OPENAI KEY\"\n",
        "llm = OpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\",retriever=retriever)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VNk_M8fR9Aep",
        "outputId": "f8233390-1053-43f3-8a3c-a5d9c3c48cbc"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:179: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/langchain/llms/openai.py:751: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain.chat_models import ChatOpenAI`\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a custom prompt template\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"\n",
        "    Use the following context to answer the question.\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "\n",
        "    Answer:\"\"\"\n",
        ")\n",
        "\n",
        "qa_chain.combine_documents_chain.llm_chain.prompt = prompt_template\n"
      ],
      "metadata": {
        "id": "ilOFgzvX9H2M"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing the System"
      ],
      "metadata": {
        "id": "H8iTp4i19KNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How are transformers used in NLP?\"\n",
        "answer = qa_chain.run(query)\n",
        "print(\"Question:\", query)\n",
        "print(\"\\nAnswer:\", answer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPRu_9Wu9LFh",
        "outputId": "53e60ea8-48b4-4000-ec06-8db7083503dd"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: How are transformers used in NLP?\n",
            "\n",
            "Answer: Transformers are used in NLP tasks to analyze and process textual data by capturing long-range dependencies and relationships between words in a sentence. Transformers have been successful in various NLP applications such as language translation, text summarization, and sentiment analysis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How are transformers used in NLP?\"\n",
        "retrieved_docs = retriever.get_relevant_documents(query)\n",
        "print(\"\\nRetrieved Documents:\" , retrieved_docs)\n",
        "for idx, doc in enumerate(retrieved_docs):\n",
        "    print(f\"\\nDocument {idx+1}:\\n{doc}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnppUL_OAsEU",
        "outputId": "e5b2658c-8ee3-46a7-9946-50f022021349"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Retrieved Documents: [Document(page_content='Transformers are a type of neural network architecture that have been used for NLP tasks.', metadata={'source_node': 'NLP'}), Document(page_content='Graph Neural Networks can capture dependencies in graph-structured data.', metadata={'source_node': 'Graph Neural Networks'}), Document(page_content='The GPT models are developed by OpenAI and are used for text generation.', metadata={'source_node': 'OpenAI'})]\n",
            "\n",
            "Document 1:\n",
            "page_content='Transformers are a type of neural network architecture that have been used for NLP tasks.' metadata={'source_node': 'NLP'}\n",
            "\n",
            "Document 2:\n",
            "page_content='Graph Neural Networks can capture dependencies in graph-structured data.' metadata={'source_node': 'Graph Neural Networks'}\n",
            "\n",
            "Document 3:\n",
            "page_content='The GPT models are developed by OpenAI and are used for text generation.' metadata={'source_node': 'OpenAI'}\n"
          ]
        }
      ]
    }
  ]
}