{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be3d6174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langgraph langchain-openai langchain-community langchain-text-splitters chromadb pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc76aedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = \"<OPENAI_KEY>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c6dc212b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyammittal/github/AI-ML/demo_applications/agent-patterns-pack/claim_verifier.py:57: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vs = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name: claim_verifier\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve()))\n",
    "\n",
    "from claim_verifier import build_retriever, make_search_tool, SYSTEM_PROMPT\n",
    "\n",
    "PDF_PATH = \"data/attention_all_u_need.pdf\"\n",
    "\n",
    "retriever = build_retriever(\n",
    "    pdf_path=PDF_PATH,\n",
    "    persist_dir=\"chroma_claim_db\",\n",
    "    collection_name=\"claim_pdf\",\n",
    "    k=5,\n",
    ")\n",
    "claim_tool = make_search_tool(retriever)\n",
    "print(\"Tool name:\", claim_tool.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4bdc144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claim_tool(query: str) -> str:\n",
    "    # Try common invoke patterns across versions\n",
    "    try:\n",
    "        return claim_tool.invoke({\"query\": query})\n",
    "    except Exception:\n",
    "        try:\n",
    "            return claim_tool.invoke(query)\n",
    "        except Exception:\n",
    "            # Fallback: call underlying function directly (rare)\n",
    "            return claim_tool.func(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c17b5c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "planner_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "executor_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "30cc46d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claim_tool(query: str) -> str:\n",
    "    # Try common invoke patterns across versions\n",
    "    try:\n",
    "        return claim_tool.invoke({\"query\": query})\n",
    "    except Exception:\n",
    "        try:\n",
    "            return claim_tool.invoke(query)\n",
    "        except Exception:\n",
    "            # Fallback: call underlying function directly (rare)\n",
    "            return claim_tool.func(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c05039b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "planner_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "executor_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cbbfea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_plan(user_question: str) -> list[str]:\n",
    "    prompt = f\"\"\"\n",
    "You are planning steps to answer a user question using a PDF search tool named `{claim_tool.name}`.\n",
    "\n",
    "User question:\n",
    "{user_question}\n",
    "\n",
    "Return a plan as 3-6 bullet points, each starting with \"- \".\n",
    "Keep steps concrete and searchable (what to look up in the policy).\n",
    "\"\"\"\n",
    "    msg = planner_llm.invoke(prompt)\n",
    "    steps = [ln[2:].strip() for ln in msg.content.splitlines() if ln.strip().startswith(\"- \")]\n",
    "    if not steps:\n",
    "        steps = [\n",
    "            \"Search the policy for the most relevant section(s)\",\n",
    "            \"Extract the key rule(s) with page citations\",\n",
    "            \"Write a final answer with citations\"\n",
    "        ]\n",
    "    return steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b990ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_plan(user_question: str, plan: list[str]) -> dict:\n",
    "    scratch = []\n",
    "\n",
    "    for i, step in enumerate(plan, start=1):\n",
    "        # Generate best search query for the step\n",
    "        q_prompt = f\"\"\"\n",
    "Generate a short search query to use with the tool `{claim_tool.name}`.\n",
    "\n",
    "User question: {user_question}\n",
    "Current step: {step}\n",
    "\n",
    "Return ONLY the query text.\n",
    "\"\"\"\n",
    "        query_msg = executor_llm.invoke(q_prompt)\n",
    "        search_query = query_msg.content.strip().strip('\"')\n",
    "\n",
    "        evidence = call_claim_tool(search_query)\n",
    "\n",
    "        # Summarize evidence for this step (keep citations)\n",
    "        s_prompt = f\"\"\"\n",
    "You are extracting useful info from cited evidence.\n",
    "\n",
    "User question: {user_question}\n",
    "Step {i}: {step}\n",
    "\n",
    "Evidence:\n",
    "{evidence}\n",
    "\n",
    "Write 2-6 bullet points of findings. Each bullet MUST include (page X) based on evidence lines.\n",
    "If evidence is irrelevant, say \"No useful evidence found.\"\n",
    "\"\"\"\n",
    "        summary_msg = executor_llm.invoke(s_prompt)\n",
    "        scratch.append(f\"## Step {i}: {step}\\nQuery: {search_query}\\n{summary_msg.content}\")\n",
    "\n",
    "    # Final answer\n",
    "    final_prompt = f\"\"\"\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Tool name available: {claim_tool.name}\n",
    "\n",
    "User question:\n",
    "{user_question}\n",
    "\n",
    "Notes from plan execution:\n",
    "{chr(10).join(scratch)}\n",
    "\n",
    "Write a FINAL answer with citations like (page X).\n",
    "If not found, write exactly: Not found in the provided PDF.\n",
    "\"\"\"\n",
    "    final_msg = executor_llm.invoke(final_prompt)\n",
    "\n",
    "    return {\n",
    "        \"plan\": plan,\n",
    "        \"scratch\": \"\\n\\n\".join(scratch),\n",
    "        \"final\": final_msg.content.strip()\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "71cfa7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PLAN:\n",
      "- Search for the term \"transformer\" in the PDF to locate sections discussing its architecture and components.\n",
      "- Look for keywords \"data softmax\" to find explanations or definitions related to softmax functions in the context of transformers.\n",
      "- Search for \"logit\" to identify any references to logits, particularly in relation to the output layer of the transformer model.\n",
      "- Check for any sections that discuss the relationship between softmax and logits, especially in terms of how they are used in the transformerâ€™s prediction process.\n",
      "- Review any examples or diagrams that illustrate the use of softmax and logits within the transformer framework for better understanding.\n",
      "\n",
      "FINAL:\n",
      " The Transformer architecture employs a learned linear transformation followed by a softmax function to convert the decoder output into predicted next-token probabilities. This process involves using learned embeddings to transform input and output tokens into vectors of dimension \\(d_{model} = 512\\) (page 4). The softmax function is crucial as it ensures that the output probabilities sum to one, creating valid probability distributions over the vocabulary. Additionally, the softmax function is applied in the decoder while masking out illegal connections to maintain the auto-regressive property, allowing predictions to be based solely on previously generated tokens (page 4). \n",
      "\n",
      "In summary, the relationship between logits and softmax in the Transformer model is that logits are the output of the learned linear transformation, which are then passed through the softmax function to produce probabilities for the next token prediction (page 4).\n"
     ]
    }
   ],
   "source": [
    "question = \"What does the transforer say about data softmax and logit?\"\n",
    "plan = make_plan(question)\n",
    "out = execute_plan(question, plan)\n",
    "\n",
    "print(\"PLAN:\")\n",
    "for s in out[\"plan\"]:\n",
    "    print(\"-\", s)\n",
    "\n",
    "print(\"\\nFINAL:\\n\", out[\"final\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
