{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a96d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU langchain langgraph langchain-openai langchain-community langchain-text-splitters chromadb pypdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1ad6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "OPENAI_API_KEY = \"<OPENAI_KEY>\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f6078eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from claim_verifier import build_retriever, make_search_tool, SYSTEM_PROMPT\n",
    "\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "042a228d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyammittal/github/AI-ML/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "/Users/satyammittal/github/AI-ML/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/satyammittal/github/AI-ML/demo_applications/agent-patterns-pack/claim_verifier.py:57: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  vs = Chroma(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tool name: claim_verifier\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path().resolve()))\n",
    "\n",
    "from claim_verifier import build_retriever, make_search_tool, SYSTEM_PROMPT\n",
    "\n",
    "PDF_PATH = \"data/attention_all_u_need.pdf\"\n",
    "\n",
    "retriever = build_retriever(\n",
    "    pdf_path=PDF_PATH,\n",
    "    persist_dir=\"chroma_claim_db\",\n",
    "    collection_name=\"claim_pdf\",\n",
    "    k=5,\n",
    ")\n",
    "claim_tool = make_search_tool(retriever)\n",
    "print(\"Tool name:\", claim_tool.name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1f7b122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claim_tool(query: str) -> str:\n",
    "    # Try common invoke patterns across versions\n",
    "    try:\n",
    "        return claim_tool.invoke({\"query\": query})\n",
    "    except Exception:\n",
    "        try:\n",
    "            return claim_tool.invoke(query)\n",
    "        except Exception:\n",
    "            # Fallback: call underlying function directly (rare)\n",
    "            return claim_tool.func(query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf67d626",
   "metadata": {},
   "outputs": [],
   "source": [
    "draft_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def draft_answer(question: str) -> dict:\n",
    "    evidence = call_claim_tool(question)\n",
    "    prompt = f\"\"\"\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Tool name: {claim_tool.name}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Evidence (cited snippets):\n",
    "{evidence}\n",
    "\n",
    "Write a DRAFT answer with citations like (page X).\n",
    "If not found, write exactly: Not found in the provided PDF.\n",
    "\"\"\"\n",
    "    draft = draft_llm.invoke(prompt).content.strip()\n",
    "    return {\"evidence\": evidence, \"draft\": draft}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2834c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "critic_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def critique(question: str, draft: str) -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are a strict reviewer for a PDF-grounded answer.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Draft:\n",
    "{draft}\n",
    "\n",
    "Check:\n",
    "1) Does every claim have a citation (page X)?\n",
    "2) Any hallucinations beyond the PDF?\n",
    "3) Is the answer incomplete or vague?\n",
    "4) Are citations plausible and consistent?\n",
    "\n",
    "Return:\n",
    "- A bullet list of issues\n",
    "- A bullet list of concrete fixes\n",
    "- A risk score from 1 (safe) to 5 (high risk)\n",
    "\"\"\"\n",
    "    return critic_llm.invoke(prompt).content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa5c38b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "revise_llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "def revise(question: str, draft: str, critique_text: str) -> str:\n",
    "    # Second retrieval guided by critique (helps a lot)\n",
    "    evidence2 = call_claim_tool(f\"{question}\\n\\nReviewer concerns:\\n{critique_text}\")\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "Tool name: {claim_tool.name}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Draft:\n",
    "{draft}\n",
    "\n",
    "Critique:\n",
    "{critique_text}\n",
    "\n",
    "More evidence:\n",
    "{evidence2}\n",
    "\n",
    "Write the FINAL improved answer with citations like (page X).\n",
    "If not found, write exactly: Not found in the provided PDF.\n",
    "\"\"\"\n",
    "    return revise_llm.invoke(prompt).content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "875b0227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/satyammittal/github/AI-ML/demo_applications/agent-patterns-pack/claim_verifier.py:91: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  hits = retriever.get_relevant_documents(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DRAFT ===\n",
      " Not found in the provided PDF.\n",
      "\n",
      "=== CRITIQUE ===\n",
      " ### Issues\n",
      "- The draft states \"Not found in the provided PDF,\" which does not address the question directly.\n",
      "- There are no citations provided, which is necessary to support any claims made.\n",
      "- The answer is incomplete as it does not provide any information about the transformer and its capabilities regarding multimodality.\n",
      "- The draft does not engage with the content of the PDF, leading to a lack of context or relevance.\n",
      "\n",
      "### Concrete Fixes\n",
      "- Review the PDF thoroughly to find relevant information about transformers and their ability to handle multimodal data.\n",
      "- Provide specific citations from the PDF (e.g., \"According to page X, transformers can process multimodal data...\").\n",
      "- Expand the answer to include a brief explanation of how transformers work with different types of data (e.g., text, images, etc.).\n",
      "- Ensure that the answer is clear and directly addresses the question posed.\n",
      "\n",
      "### Risk Score\n",
      "4 (high risk) - The answer is not only incomplete but also lacks citations and fails to engage with the content of the PDF, which could lead to misinformation or misinterpretation of the material.\n",
      "\n",
      "=== FINAL ===\n",
      " Not found in the provided PDF.\n"
     ]
    }
   ],
   "source": [
    "q = \"Does the transformer allow multimodality of data?\"\n",
    "\n",
    "d = draft_answer(q)\n",
    "c = critique(q, d[\"draft\"])\n",
    "final = revise(q, d[\"draft\"], c)\n",
    "\n",
    "print(\"=== DRAFT ===\\n\", d[\"draft\"])\n",
    "print(\"\\n=== CRITIQUE ===\\n\", c)\n",
    "print(\"\\n=== FINAL ===\\n\", final)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdedca6e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6acc24fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DRAFT ===\n",
      " Yes, the transformer uses the softmax function. It is mentioned that a learned linear transformation and softmax function are used to convert the decoder output to predicted next-token probabilities (page 4).\n",
      "\n",
      "=== CRITIQUE ===\n",
      " ### Issues\n",
      "- The claim about the transformer using the softmax function is made, but the citation is vague (\"page 4\") without specifying the document title or context.\n",
      "- The answer does not clarify which part of the transformer architecture utilizes the softmax function, potentially leading to confusion.\n",
      "- There is no mention of the context in which the softmax function is applied (e.g., during the attention mechanism or output layer).\n",
      "- The phrase \"mentioned that a learned linear transformation and softmax function\" is somewhat vague and could be more precise.\n",
      "\n",
      "### Concrete Fixes\n",
      "- Specify the document title or context when citing \"page 4\" to provide clarity.\n",
      "- Elaborate on where in the transformer architecture the softmax function is applied (e.g., in the output layer of the decoder).\n",
      "- Clarify the role of the learned linear transformation in conjunction with the softmax function.\n",
      "- Rephrase the answer to enhance clarity and precision, such as: \"Yes, the transformer architecture employs the softmax function in the output layer of the decoder to convert the logits from a learned linear transformation into predicted probabilities for the next token (see [Document Title], page 4).\"\n",
      "\n",
      "### Risk Score\n",
      "3 (moderate risk) - The answer contains some accurate information but lacks clarity and specificity, which could lead to misunderstandings.\n",
      "\n",
      "=== FINAL ===\n",
      " Yes, the transformer architecture employs the softmax function in the output layer of the decoder to convert the logits from a learned linear transformation into predicted probabilities for the next token (page 4). Additionally, the softmax function is also applied in the scaled dot-product attention mechanism to compute the weights on the values based on the compatibility of queries and keys (page 3).\n"
     ]
    }
   ],
   "source": [
    "q = \"Does the transformer use softmax?\"\n",
    "\n",
    "d = draft_answer(q)\n",
    "c = critique(q, d[\"draft\"])\n",
    "final = revise(q, d[\"draft\"], c)\n",
    "\n",
    "print(\"=== DRAFT ===\\n\", d[\"draft\"])\n",
    "print(\"\\n=== CRITIQUE ===\\n\", c)\n",
    "print(\"\\n=== FINAL ===\\n\", final)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
